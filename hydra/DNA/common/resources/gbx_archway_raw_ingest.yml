# The main job for raw.
resources:
  jobs:
    gbx_archway_raw_ingest:
      name: gbx_archway_raw_ingest
      max_concurrent_runs: 1

      continuous:
        pause_status: ${var.schedule_status}
        task_retry_mode: ON_FAILURE

      webhook_notifications:
        on_failure:
          - id: ${var.slack_webhook}

      tasks:
        - task_key: gbx_archway_raw_ingest
          job_cluster_key: gbx_archway_ingest_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/streaming/raw/s3_stream_gbx_raw_ingest.py
            base_parameters:
              environment: ${var.environment}
              checkpoint_location: dbfs:/tmp/gbx/raw/run${var.environment}
              title: gbx
              s3_bucket: 2k-ana-prd-gbx-ccu
              target_catalog: gbx${var.environment}
              target_schema: raw
              s3_prefix: 'version=3/stream=game/*'
        
        - task_key: gbx_archway_flatten_logins
          job_cluster_key: gbx_archway_ingest_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/streaming/raw/gbx_archway_flatten.py
            base_parameters:
              environment: ${var.environment}
              checkpoint_location: dbfs:/tmp/gbx/archway/raw/run${var.environment}
              title: gbx
              target_schema: raw
              
      job_clusters:
        - job_cluster_key: gbx_archway_ingest_cluster${var.environment}
          new_cluster:
            spark_version: 16.4.x-scala2.12
            data_security_mode: SINGLE_USER
            aws_attributes:
              availability: ON_DEMAND
#EBS properties will help to deploy the bundle successfully 
#At least one EBS volume must be attached for clusters created with node type m7g.xlarge.              
              ebs_volume_type: GENERAL_PURPOSE_SSD
              ebs_volume_count: 1
              ebs_volume_size: 100
            node_type_id: m7g.xlarge
            driver_node_type_id: m7g.xlarge
            num_workers: 1
            init_scripts:
              - workspace:
                  destination: /Shared/datadog/dd_init.sh
            spark_env_vars:
              DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
            custom_tags:
              owner: "Data Engineering"
      
      tags:
        business_unit: "Core Games"
        title: "GBX Archway"
        data_layer: "raw"
        environment: ${var.environment}
        owner: "Data Engineering"
      
      permissions:
        - level: CAN_MANAGE
          group_name: "dna live"
        - level: CAN_MANAGE
          group_name: "data engineering"

