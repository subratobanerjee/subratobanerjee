# The main job for batch managed.
resources:
  jobs:
    reference_customer_daily:
      name: reference_customer_daily
      max_concurrent_runs: 1

      webhook_notifications:
        on_failure:
          - id: ${var.slack_webhook}

      schedule:
        quartz_cron_expression: 0 0 8 * * ?
        timezone_id: UTC
        pause_status: ${var.schedule_status}

      tasks:
        - task_key: gbx_coppa_logins
          job_cluster_key: reference_customer_daily_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/customer/gbx_coppa_logins.py
            base_parameters:
              inputParam: '{"environment":"${var.environment}"}'

      job_clusters:
        - job_cluster_key: reference_customer_daily_cluster${var.environment}
          new_cluster:
            spark_version: 15.2.x-scala2.12
            aws_attributes:
              availability: ON_DEMAND
            node_type_id: ${var.node_type_id}
            driver_node_type_id: ${var.driver_node_type_id}
            autoscale:
              min_workers: ${var.num_workers}
              max_workers: ${var.max_num_workers}
            init_scripts:
              - workspace:
                  destination: /Shared/datadog/dd_init.sh
            spark_env_vars:
              DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
            custom_tags:
              owner: "Data Engineering"
      
      tags:
        business_unit: "Customer"
        title: "reference"
        catalog: "reference${var.environment}"
        data_layer: "customer"
        environment: ${var.environment}

      permissions:
        - level: CAN_MANAGE
          group_name: "dna live"
        - level: CAN_MANAGE
          group_name: "data engineering"