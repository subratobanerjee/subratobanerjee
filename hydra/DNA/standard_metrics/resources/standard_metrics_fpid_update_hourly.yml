# The main job for batch managed.
resources:
  jobs:
    standard_metrics_fpid_update_hourly:
      name: standard_metrics_fpid_update_hourly
      max_concurrent_runs: 1

      webhook_notifications:
        on_failure:
          - id: ${var.slack_webhook}
      
      schedule:
        quartz_cron_expression: 0 15 5,12 * * ? *
        timezone_id: UTC
        pause_status: ${var.schedule_status}

      tasks:
        # - task_key: nba_fact_player_ltd
        #   job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
        #   notebook_task:
        #     notebook_path: ../src/batch/managed/fpid_update/nba_fact_player_ltd.py
        #     base_parameters:
        #       inputParam: '{"environment":"${var.environment}"}'
        #       checkpoint: 'dbfs:/tmp/standard_metrics/managed/fpid_update/batch/run${var.environment}/nba_fact_player_ltd'

        # - task_key: nbapg_fact_player_ltd
        #   job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
        #   notebook_task:
        #     notebook_path: ../src/batch/managed/fpid_update/nbapg_fact_player_ltd.py
        #     base_parameters:
        #       inputParam: '{"environment":"${var.environment}"}'
        #       checkpoint: 'dbfs:/tmp/standard_metrics/managed/batch/fpid_update/run${var.environment}/nbapg_fact_player_ltd'
        #   depends_on:
        #     - task_key: nba_fact_player_ltd


        - task_key: fact_player_ltd
          job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/batch/managed/fpid_update/fact_player_ltd.py
            base_parameters:
              inputParam: '{"environment":"${var.environment}"}'
              checkpoint: 'dbfs:/tmp/standard_metrics/managed/batch/fpid_update/run${var.environment}/fact_player_ltd'
          # depends_on:
          #   - task_key: nbapg_fact_player_ltd

        # - task_key: wwe_fact_player_ltd
        #   job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
        #   notebook_task:
        #     notebook_path: ../src/batch/managed/fpid_update/wwe_fact_player_ltd.py
        #     base_parameters:
        #       inputParam: '{"environment":"${var.environment}"}'
        #       checkpoint: 'dbfs:/tmp/standard_metrics/managed/batch/fpid_update/run${var.environment}/wwe_fact_player_ltd'
        #   depends_on:
        #     - task_key: fact_player_ltd
            
        - task_key: fact_login
          job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/batch/managed/fpid_update/fact_login.py
            base_parameters:
              inputParam: '{"environment":"${var.environment}"}'
              checkpoint: 'dbfs:/tmp/standard_metrics/managed/fpid_update/batch/run${var.environment}/fact_login'
          depends_on:
            - task_key: fact_player_ltd

        - task_key: fact_player_ltd_gdpr
          job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/batch/managed/fpid_update/fact_player_ltd_gdpr.py
            base_parameters:
              inputParam: '{"environment":"${var.environment}"}'
              checkpoint: 'dbfs:/tmp/standard_metrics/managed/batch/fpid_update/run${var.environment}/fact_player_ltd_gdpr'
          depends_on:
            - task_key: fact_login
        
        - task_key: fact_login_gdpr
          job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
          notebook_task:
            notebook_path: ../src/batch/managed/fpid_update/fact_login_gdpr.py
            base_parameters:
              inputParam: '{"environment":"${var.environment}"}'
              checkpoint: 'dbfs:/tmp/standard_metrics/managed/batch/fpid_update/run${var.environment}/fact_login_gdpr'
          depends_on:
            - task_key: fact_login

      # The managed cluster will use memory-optimized instances due to having to store state in memory
      job_clusters:
        - job_cluster_key: standard_metrics_fpid_update_cluster${var.environment}
          new_cluster:
            spark_version: 16.4.x-scala2.12
            data_security_mode: SINGLE_USER
            aws_attributes:
              availability: ON_DEMAND
            node_type_id: ${var.node_type_id}
            driver_node_type_id: ${var.driver_node_type_id}
            num_workers: ${var.num_workers}
            init_scripts:
              - workspace: 
                  destination: /Shared/datadog/dd_init.sh
            spark_conf:
              'spark.driver.extraJavaOptions': '-XX:+UseZGC'
            spark_env_vars:
              DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
            custom_tags:
              owner: "Data Engineering"
        
      
      tags:
        business_unit: "Standard Metrics"
        catalog: "dataanalytics${var.environment}"
        data_layer: "managed"
        environment: ${var.environment}
        owner: "Data Engineering"

      permissions:
        - level: CAN_MANAGE
          group_name: "dna live"
        - level: CAN_MANAGE
          group_name: "data engineering"