# # The main job for raw.
# resources:
#   jobs:
#     nero_managed_nrt:
#       name: nero_managed_nrt
#       max_concurrent_runs: 1

#       continuous:
#         pause_status: ${var.schedule_status}

#       webhook_notifications:
#         on_failure:
#           - id: ${var.slack_webhook}

#       tasks:
        

#         - task_key: agg_installs_nrt
#           job_cluster_key: nero_streaming${var.environment}
#           notebook_task:
#             notebook_path: ../nero/src/streaming/managed/agg_installs_nrt.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#               checkpoint: 'dbfs:/tmp/nero/managed/streaming/run${var.environment}/agg_installs_nrt'

#         - task_key: agg_login_nrt
#           job_cluster_key: nero_streaming${var.environment}
#           notebook_task:
#             notebook_path: ../nero/src/streaming/managed/agg_login_nrt.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#               checkpoint: 'dbfs:/tmp/nero/managed/streaming/run${var.environment}/agg_login_nrt'

#         - task_key: agg_mission_starts_nrt
#           job_cluster_key: nero_streaming${var.environment}
#           notebook_task:
#             notebook_path: ../nero/src/streaming/managed/agg_mission_starts_nrt.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'

#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}



#       job_clusters:
#         - job_cluster_key: nero_streaming${var.environment}
#           new_cluster:
#             spark_version: 16.4.x-scala2.12
#             aws_attributes:
#               availability: ON_DEMAND
#             node_type_id: ${var.node_type_id}
#             driver_node_type_id: ${var.driver_node_type_id}
#             num_workers: ${var.num_workers}
#             enable_elastic_disk: true
#             init_scripts:
#               - workspace:
#                   destination: /Shared/datadog/dd_init.sh
#             spark_conf:
#               'spark.driver.extraJavaOptions': '-XX:+UseZGC'
#             spark_env_vars:
#               DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
#             custom_tags:
#               owner: "Data Engineering"

#       tags:
#         business_unit: "Core Games"
#         title: "Nero"
#         data_layer: "raw"
#         environment: ${var.environment}
#         owner: "Data Engineering"

#       permissions:
#         - level: CAN_MANAGE
#           group_name: "dna live"
#         - level: CAN_MANAGE
#           group_name: "data engineering"