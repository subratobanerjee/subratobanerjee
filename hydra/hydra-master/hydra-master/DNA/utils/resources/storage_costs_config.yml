# The main job for storage cost.
resources:
  jobs:
    storage_cost_compute:
      name: storage_cost_compute
      max_concurrent_runs: 1
      
      email_notifications:
        on_failure:
          - tarun.kumar@2k.com
          - kevin.ewert@2k.com
      
      webhook_notifications:
        on_failure:
          - id: ${var.slack_webhook}

      schedule:
        quartz_cron_expression: 0 0 * ? * * *
        timezone_id: UTC
        pause_status: ${var.schedule_status}

      tasks:
        - task_key: storage_cost_compute_task
          job_cluster_key: storage_cost_compute_cluster${var.environment}
          python_wheel_task:
            package_name: utils
            entry_point: compute_storage_cost
            parameters:
              - --environment
              - ${var.environment}
              - --catalog_list
              - horizon,coretech,bluenose,inverness
              - --schema_list
              - '{"horizon": ["raw", "managed"], "coretech": ["sso"],"bluenose": ["raw","managed"],"inverness": ["raw","managed"]}'
              - --target_catalog
              - ${var.target_catalog}${var.environment}
              - --target_schema
              - utils
              - --target_table
              - storage_cost_db
              - --cost_per_gb
              - 0.023
          libraries:
            - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.1-py3-none-any.whl     
      parameters:
        - name: environment
          default: ${var.environment}
        - name: checkpoint_location
          default: dbfs:/tmp/utils/storage_costs/run${var.environment}

      # The managed cluster will use memory-optimized instances due to having to store state in memory
      job_clusters:
        - job_cluster_key: storage_cost_compute_cluster${var.environment}
          new_cluster:
            spark_version: 15.2.x-scala2.12
            data_security_mode: SINGLE_USER
            aws_attributes:
              availability: ON_DEMAND
            node_type_id: ${var.node_type_id}
            driver_node_type_id: ${var.driver_node_type_id}
            num_workers: ${var.num_workers}
            init_scripts:
              - workspace: 
                  destination: /Shared/datadog/dd_init.sh
            spark_env_vars:
              DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
            custom_tags:
              owner: "Data Engineering"
      
      tags:
        business_unit: "utils"

      permissions:
        - level: CAN_MANAGE
          group_name: "dna live"
        - level: CAN_MANAGE
          group_name: "data engineering"