# The main job for raw.
resources:
  jobs:
    inverness_raw_ingest:
      name: inverness_raw_ingest
      max_concurrent_runs: 1

      continuous:
        pause_status: ${var.schedule_status}
        task_retry_mode: ON_FAILURE

      webhook_notifications:
        on_failure:
          - id: ${var.slack_webhook}

      tasks:
        - task_key: inverness_raw_ingest
          job_cluster_key: inverness_ingest_cluster${var.environment}
          #notebook_task:
          #notebook_path: ../../../../streaming/common/raw/common_raw_ingest.py
          python_wheel_task:
            package_name: common
            entry_point: stream_raw_ingest
            parameters:
              - --environment
              - ${var.environment}
              - --checkpoint_location
              - dbfs:/tmp/inverness/raw/run${var.environment}
              - --bootstrap_servers
              - pkc-3nrx70.us-east-1.aws.confluent.cloud:9092
              - --title
              - inverness
              - --topics
              - telemetry.events.read.good.417574d161094411940af555f60552e2.1,telemetry.events.read.good.9bd01e5b8d924235aeb50423c88a70f5.1,telemetry.events.read.good.fabf04782e974cf0903648e127977830.1,telemetry.events.read.good.5ec9a4a1c721404d9296821fec929aa6.1
              - --starting_offsets
              - earliest
          webhook_notifications:
            on_failure:
              - id: ${var.slack_webhook}
          libraries:
            - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.6-py3-none-any.whl

      job_clusters:
        - job_cluster_key: inverness_ingest_cluster${var.environment}
          new_cluster:
            spark_version: 16.2.x-scala2.12
            data_security_mode: SINGLE_USER
            aws_attributes:
              availability: ON_DEMAND
            node_type_id: ${var.node_type_id}
            driver_node_type_id: ${var.driver_node_type_id}
            num_workers: ${var.num_workers}
            init_scripts:
              - workspace:
                  destination: /Shared/datadog/dd_init.sh
            spark_conf:
              'spark.driver.extraJavaOptions': '-XX:+UseZGC'
            spark_env_vars:
              DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
            custom_tags:
              owner: "Data Engineering"
      
      tags:
        business_unit: "Core Games"
        title: "Inverness"
        data_layer: "raw"
        environment: ${var.environment}
        owner: "Data Engineering"
      
      permissions:
        - level: CAN_MANAGE
          group_name: "dna live"
        - level: CAN_MANAGE
          group_name: "data engineering"