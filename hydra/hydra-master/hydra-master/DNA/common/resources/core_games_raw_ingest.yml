# # The main job for raw.
# resources:
#   jobs:
#     core_games_ingest:
#       name: core_games_ingest
#       max_concurrent_runs: 1

#       continuous:
#         pause_status: ${var.schedule_status}

#       webhook_notifications:
#         on_failure:
#           - id: ${var.slack_webhook}

#       tasks:
#         - task_key: venice_raw_ingest
#           job_cluster_key: core_games_ingest_cluster${var.environment}
#           #notebook_task:
#             #notebook_path: ../../../../streaming/common/raw/common_raw_ingest.py
#           python_wheel_task:
#             package_name: common
#             entry_point: stream_raw_ingest
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/venice/raw/run${var.environment}
#               - --bootstrap_servers
#               - pkc-3nrx70.us-east-1.aws.confluent.cloud:9092
#               - --title
#               - venice
#               - --topics
#               - telemetry.events.read.good.020cfd6eb88c426faf65bef16ede5d4d.1,telemetry.events.read.good.269e372df50f4f6d91ba1caf1d2b9e61.1
#               - --starting_offsets
#               - earliest
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}
#           libraries:
#             - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.6-py3-none-any.whl

#         - task_key: inverness_raw_ingest
#           job_cluster_key: core_games_ingest_cluster${var.environment}
#           #notebook_task:
#           #notebook_path: ../../../../streaming/common/raw/common_raw_ingest.py
#           python_wheel_task:
#             package_name: common
#             entry_point: stream_raw_ingest
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/inverness/raw/run${var.environment}
#               - --bootstrap_servers
#               - pkc-3nrx70.us-east-1.aws.confluent.cloud:9092
#               - --title
#               - inverness
#               - --topics
#               - telemetry.events.read.good.417574d161094411940af555f60552e2.1,telemetry.events.read.good.9bd01e5b8d924235aeb50423c88a70f5.1
#               - --starting_offsets
#               - earliest
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}
#           libraries:
#             - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.6-py3-none-any.whl

#         - task_key: parkside_raw_ingest
#           job_cluster_key: core_games_ingest_cluster${var.environment}
#           #notebook_task:
#             #notebook_path: ../../../../streaming/common/raw/common_raw_ingest.py
#           python_wheel_task:
#             package_name: common
#             entry_point: stream_raw_ingest
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/parkside/raw/run${var.environment}
#               - --bootstrap_servers
#               - pkc-3nrx70.us-east-1.aws.confluent.cloud:9092
#               - --title
#               - parkside
#               - --topics
#               - telemetry.events.read.good.73d946138d9b4c12ae047613f85ce277.1,telemetry.events.read.good.93705006895940008c5a9b586694b88f.1
#               - --starting_offsets
#               - earliest
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}
#           libraries:
#             - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.6-py3-none-any.whl

#         - task_key: nero_raw_ingest
#           job_cluster_key: core_games_ingest_cluster${var.environment}
#           #notebook_task:
#             #notebook_path: ../../../../streaming/common/raw/common_raw_ingest.py
#           python_wheel_task:
#             package_name: common
#             entry_point: stream_raw_ingest
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/nero/raw/run${var.environment}
#               - --bootstrap_servers
#               - pkc-3nrx70.us-east-1.aws.confluent.cloud:9092
#               - --title
#               - nero
#               - --topics
#               - telemetry.events.read.good.c818ad1d80dd4638961b95d154151402.1
#               - --starting_offsets
#               - earliest
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}
#           libraries:
#             - whl: /Workspace/Users/${workspace.current_user.userName}/${bundle.name}/${bundle.target}/artifacts/.internal/${var.wheel_name}-0.6-py3-none-any.whl


#       job_clusters:
#         - job_cluster_key: core_games_ingest_cluster${var.environment}
#           new_cluster:
#             spark_version: 16.0.x-scala2.12
#             aws_attributes:
#               availability: ON_DEMAND
#             node_type_id: ${var.node_type_id}
#             driver_node_type_id: ${var.driver_node_type_id}
#             num_workers: ${var.num_workers}
#             init_scripts:
#               - workspace:
#                   destination: /Shared/datadog/dd_init.sh
#             spark_conf:
#               'spark.driver.extraJavaOptions': '-XX:+UseZGC'
#             spark_env_vars:
#               DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
#             custom_tags:
#               owner: "Data Engineering"

#       tags:
#         business_unit: "Core Games"
#         data_layer: "raw"
#         environment: ${var.environment}
#         owner: "Data Engineering"

#       permissions:
#         - level: CAN_MANAGE
#           group_name: "dna live"
#         - level: CAN_MANAGE
#           group_name: "data engineering"
