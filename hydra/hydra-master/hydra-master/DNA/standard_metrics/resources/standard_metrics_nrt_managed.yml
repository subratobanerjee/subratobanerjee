# # The main job for raw.
# resources:
#   jobs:
#     standard_metrics_managed_nrt:
#       name: standard_metrics_managed_nrt
#       max_concurrent_runs: 1

#       continuous:
#         pause_status: ${var.schedule_status}

#       webhook_notifications:
#         on_failure:
#           - id: ${var.slack_webhook}

#       tasks:
#         - task_key: agg_account_link_10min
#           job_cluster_key: standard_metrics_managed_nrt_cluster${var.environment}
#           python_wheel_task:
#             package_name: standard_metrics
#             entry_point: stream_account_link
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/standard_metrics/managed/streaming/run${var.environment}/agg_account_link_10min
#           libraries:
#             - whl: ${var.wheel_path}
            
#         - task_key: agg_web_event_10min
#           job_cluster_key: standard_metrics_managed_nrt_cluster${var.environment}
#           python_wheel_task:
#             package_name: standard_metrics
#             entry_point: stream_web_events
#             parameters:
#               - --environment
#               - ${var.environment}
#               - --checkpoint_location
#               - dbfs:/tmp/standard_metrics/managed/streaming/run${var.environment}/agg_web_event_10min
#           libraries:
#             - whl: ${var.wheel_path}
   
#       parameters:
#         - name: environment
#           default: ${var.environment}
#         - name: checkpoint_location
#           default: dbfs:/tmp/standard_metrics/managed/streaming/run${var.environment}

#       # The managed cluster will use memory-optimized instances due to having to store state in memory
#       job_clusters:
#         - job_cluster_key: standard_metrics_managed_nrt_cluster${var.environment}
#           new_cluster:
#             spark_version: 15.2.x-scala2.12
#             aws_attributes:
#               availability: ON_DEMAND
#             node_type_id: ${var.node_type_id}
#             driver_node_type_id: ${var.driver_node_type_id}
#             num_workers: ${var.num_workers}
#             init_scripts:
#               - workspace: 
#                   destination: /Shared/datadog/dd_init.sh
#             spark_env_vars:
#               DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
#             custom_tags:
#               owner: "Data Engineering"
      
#       tags:
#         business_unit: "Standard Metrics"
#         catalog: "dataanalytics${var.environment}"
#         data_layer: "managed"
#         environment: ${var.environment}
#         owner: "Data Engineering"

#       permissions:
#         - level: CAN_MANAGE
#           group_name: "dna live"
#         - level: CAN_MANAGE
#           group_name: "data engineering"
