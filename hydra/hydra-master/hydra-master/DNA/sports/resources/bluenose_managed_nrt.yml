# # The main job for batch managed.
# resources:
#   jobs:
#     bluenose_managed_nrt:
#       name: bluenose_managed_nrt
#       max_concurrent_runs: 1

#       continuous:
#         pause_status: ${var.schedule_status}

#       tasks:

#         - task_key: fact_player_activity
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/intermediate/fact_player_activity.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook} 

#         - task_key: fact_player_session
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/intermediate/fact_player_session.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}

#         - task_key: fact_player_transaction
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/intermediate/fact_player_transaction.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}

#         - task_key: fact_player_summary_ltd
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/managed/fact_player_summary_ltd.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}

#         - task_key: fact_player_igo
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/intermediate/fact_player_igo.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#               checkpoint: 'dbfs:/tmp/bluenose/intermediate/streaming/run${var.environment}/fact_player_igo'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook} 
#         - task_key: fact_player_lesson
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/managed/fact_player_lesson.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#               checkpoint: 'dbfs:/tmp/bluenose/managed/streaming/run${var.environment}/fact_player_lesson'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}

#         - task_key: fact_player_entitlement
#           job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           notebook_task:
#             notebook_path: ../bluenose/src/streaming/intermediate/fact_player_entitlement.py
#             base_parameters:
#               inputParam: '{"environment":"${var.environment}"}'
#           webhook_notifications:
#             on_failure:
#               - id: ${var.slack_webhook}

#       job_clusters:
#         - job_cluster_key: bluenose_managed_nrt_cluster${var.environment}
#           new_cluster:
#             spark_version: 15.2.x-scala2.12
#             aws_attributes:
#               availability: ON_DEMAND
#             node_type_id: ${var.node_type_id}
#             driver_node_type_id: ${var.driver_node_type_id}
#             autoscale:
#               min_workers: ${var.num_workers_bluenose}
#               max_workers: ${var.max_workers_bluenose}
#             init_scripts:
#               - workspace:
#                   destination: /Shared/datadog/dd_init.sh
#             spark_env_vars:
#               DD_API_KEY: "{{secrets/data-engineering/dd_api_key}}"
#             custom_tags:
#               owner: "Data Engineering"
      
#       tags:
#         business_unit: "Sports"
#         data_layer: "managed"
#         environment: ${var.environment}
#         owner: "Data Engineering"
      
#       permissions:
#         - level: CAN_MANAGE
#           group_name: "dna live"
#         - level: CAN_MANAGE
#           group_name: "data engineering"
