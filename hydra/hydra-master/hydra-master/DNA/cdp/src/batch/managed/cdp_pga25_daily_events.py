# Databricks notebook source
# MAGIC %run ../../../../../utils/set_spark_session

# COMMAND ----------

# MAGIC %run ../../../../../utils/helpers

# COMMAND ----------

# MAGIC %run ../../../../../utils/ddl/table_functions

# COMMAND ----------

from pyspark.sql.functions import expr
from delta.tables import DeltaTable
from datetime import date, timedelta

# COMMAND ----------
spark = create_spark_session(name="pga2k25_daily_events")

# COMMAND ----------
input_param = dbutils_input_params()
environment = input_param.get('environment', set_environment())
load_type = input_param.get('load_type', 'incremental').lower()
incremental_days = int(input_param.get('incremental_days', '2'))

# COMMAND ----------
target_table = f"dataanalytics{environment}.cdp_ng.pga2k25_daily_events_table"


# COMMAND ----------
def create_pga2k25_daily_events(environment, spark):
    sql = f"""
            CREATE TABLE IF NOT EXISTS {target_table} (
            EVENT_DATE DATE,
            BRAND_FIRSTPARTYID STRING,
            SALTED_PUID STRING,
            PUID STRING,
            VENDOR STRING,
            GAME_TITLE STRING,
            SUB_GAME STRING,
            EVENT_NAME STRING,
            EVENT_ACTION STRING,
            EVENT_RESULT STRING,
            event_description STRING,
            CDP_FIELD_NAME STRING,
            dw_insert_ts timestamp comment 'data warehouse audit field for records inserted timestamp',
            dw_update_ts timestamp comment 'data warehouse audit field for records updated timestamp',
            merge_key string comment 'unique id generated by the hash of the grain of the table '
            )"""
    properties = {
        "delta.feature.allowColumnDefaults": "supported"
    }
    create_table(spark, sql, properties)

# COMMAND ----------

def extract(spark, load_type, incremental_days, max_date):
    incremental_filter = f"CAST(received_on AS DATE) >= '{max_date - timedelta(days=incremental_days)}'"
    igc_incremental_filter = f"CAST(date AS DATE) >= '{max_date - timedelta(days=incremental_days)}'"
    backlog_cutoff = "2025-02-20T08:50:00+00:00"

    services_df = spark.table("REFERENCE_CUSTOMER.PLATFORM.SERVICE")
    platform_df = spark.table("REFERENCE_CUSTOMER.PLATFORM.PLATFORM")
    link_event_df = spark.table(f"coretech{environment}.sso.linkevent")
    dim_title_df = spark.table(f"reference{environment}.title.dim_title")
    ctp_profile_df = spark.table("reference_customer.customer.vw_ctp_profile")
    clubpass_purchase_df = spark.table("sf_databricks_migration.sf_dbx.bluenose_clubpass_purchase")
    pga_summary_df = spark.table(f"dataanalytics{environment}.cdp_ng.vw_pga2k25_summary")
    seasonpass_purchase_df = spark.table(f"bluenose{environment}.managed_view.fact_player_seasonpass_purchase")

    player_activity_primary = spark.table(f"bluenose{environment}.intermediate.fact_player_activity")
    if load_type == 'full':
        player_activity_backlog = spark.table(f"bluenose{environment}.intermediate_bkup.fact_player_activity")
        player_activity_df = player_activity_primary.filter(f"received_on >= '{backlog_cutoff}'") \
            .unionByName(player_activity_backlog.filter(f"received_on < '{backlog_cutoff}'"))
    else:
        player_activity_df = player_activity_primary.filter(incremental_filter)

    igc_purchase_primary = spark.table(f"bluenose{environment}.managed_view.fact_player_igc_purchase_daily")
    if load_type == 'full':
        igc_purchase_backlog = spark.table(f"bluenose{environment}.managed_bkup.fact_player_igc_purchase_daily_view")
        igc_purchase_df = igc_purchase_primary.filter(f"CAST(date AS TIMESTAMP) >= '{backlog_cutoff}'") \
            .unionByName(igc_purchase_backlog.filter(f"CAST(date AS TIMESTAMP) < '{backlog_cutoff}'"))
    else:
        igc_purchase_df = igc_purchase_primary.filter(igc_incremental_filter)

    return {
        "services": services_df, "platform": platform_df, "player_activity": player_activity_df,
        "igc_purchase": igc_purchase_df, "link_event": link_event_df, "dim_title": dim_title_df,
        "ctp_profile": ctp_profile_df, "clubpass_purchase": clubpass_purchase_df,
        "pga_summary": pga_summary_df, "seasonpass_purchase": seasonpass_purchase_df
    }

# COMMAND ----------
def transform(dataframes):

    services = dataframes["services"].selectExpr(
        "CASE WHEN SERVICE = 'xbl' THEN 'Xbox Live' ELSE SERVICE END as JOIN_KEY",
        "SERVICE", "VENDOR"
    ).distinct()

    player_window_spec = "PARTITION BY pa.player_id ORDER BY CAST(pa.received_on AS TIMESTAMP) DESC"
    fact_players_ltd = dataframes["player_activity"].alias("pa") \
        .join(services.alias("s"), expr("lower(s.JOIN_KEY) = lower(pa.service)")) \
        .join(dataframes["platform"].alias("p"), expr("pa.platform = p.src_platform")) \
        .where("pa.platform IS NOT NULL") \
        .selectExpr(
            "lower(pa.player_id) as player_id",
            "CAST(pa.received_on AS DATE) as player_dates",
            "pa.extra_info_5 as install_type",
            f"first_value(s.VENDOR, true) OVER ({player_window_spec}) as vendor",
            f"first_value(s.SERVICE, true) OVER ({player_window_spec}) as service"
        ).distinct()

    fl_install = fact_players_ltd.where("install_type = 'demo'") \
        .groupBy("player_id", "vendor") \
        .agg(expr("min(player_dates) as EVENT_DATE"), expr("'fl_install' as EVENT_NAME"), expr("1 as EVENT_RESULT"))

    fg_install = fact_players_ltd.where("install_type = 'full game'") \
        .groupBy("player_id", "vendor") \
        .agg(expr("min(player_dates) as EVENT_DATE"), expr("'fg_install' as EVENT_NAME"), expr("1 as EVENT_RESULT"))
    
    reactivation_base = fact_players_ltd.groupBy("player_id", "vendor").agg(expr("min(player_dates) as EVENT_DATE"))
    reactivation = reactivation_base \
        .withColumn("prev_date", expr("lag(EVENT_DATE, 1) OVER (PARTITION BY player_id ORDER BY EVENT_DATE)")) \
        .where("datediff(EVENT_DATE, prev_date) >= 14") \
        .selectExpr("EVENT_DATE", "player_id", "vendor", "'reactivation' as EVENT_NAME", "1 as EVENT_RESULT")

    bought_vc = dataframes["igc_purchase"].alias("igc") \
        .join(dataframes["platform"].alias("p"), expr("igc.PLATFORM = p.src_platform")) \
        .where("igc.sku_type = 'VC'") \
        .withColumn("event_date_col", expr("CAST(igc.date AS DATE)")) \
        .groupBy("event_date_col", "igc.player_id", "p.VENDOR") \
        .agg(expr("sum(igc.dollar_total_amount) as EVENT_RESULT")) \
        .where("EVENT_RESULT > 0") \
        .selectExpr(
            "event_date_col as EVENT_DATE",
            "player_id",
            "VENDOR as vendor",
            "'bought_vc' as EVENT_NAME",
            "'purchase' as EVENT_ACTION",
            "EVENT_RESULT"
        )

    recurrent_spend = fact_players_ltd.alias("fp") \
        .join(dataframes["igc_purchase"].alias("trans"), expr("fp.player_id = trans.player_id"), "left") \
        .where("trans.dollar_total_amount > 0") \
        .join(dataframes["seasonpass_purchase"].alias("s"), expr("fp.player_id = s.player_id"), "left") \
        .withColumn("event_date_col", expr("CAST(trans.date AS DATE)")) \
        .groupBy("event_date_col", "trans.player_id", "fp.vendor") \
        .agg(expr("SUM(trans.dollar_total_amount) AS EVENT_DESCRIPTION")) \
        .selectExpr(
            "event_date_col as EVENT_DATE",
            "player_id",
            "vendor",
            "'recurrent_consumer_spend' as EVENT_NAME",
            "'rcs' as EVENT_ACTION",
            "1 as EVENT_RESULT",
            "EVENT_DESCRIPTION"
        )

    clubpass_purchase = dataframes["clubpass_purchase"].alias("pur") \
        .join(fact_players_ltd.alias("fp"), expr("pur.player_id = fp.player_id"), "inner") \
        .selectExpr(
            "pur.EVENT_DATE", "pur.player_id", "fp.vendor",
            "pur.REQUIRED_SEASON_PASS AS EVENT_NAME",
            "CASE WHEN pur.IS_PREMIUM THEN 'premium' ELSE 'non-premium' END AS EVENT_ACTION",
            "1 AS EVENT_RESULT"
        )

    all_events_unioned = fl_install.unionByName(fg_install, allowMissingColumns=True) \
        .unionByName(reactivation, allowMissingColumns=True) \
        .unionByName(bought_vc, allowMissingColumns=True) \
        .unionByName(recurrent_spend, allowMissingColumns=True) \
        .unionByName(clubpass_purchase, allowMissingColumns=True)
                           
    all_events = all_events_unioned.selectExpr(
        "EVENT_DATE", "player_id", "vendor", "'pga2k25' as GAME_TITLE",
        "'total' as SUB_GAME", "EVENT_NAME", "EVENT_ACTION", "EVENT_RESULT", "EVENT_DESCRIPTION"
    )

    link_event_base = dataframes["link_event"].join(dataframes["dim_title"], expr("appId = app_id")) \
        .where("title LIKE '%2K Portal%' AND firstpartyId IS NOT NULL AND firstpartyId != 'None' AND firstpartyId NOT LIKE '%@%'")

    fpids_1 = link_event_base.where("accounttype = 2 AND targetaccounttype = 3") \
        .selectExpr("accountid as platform_id", f"dataanalytics{environment}.cdp_ng.fpid_decryption(firstpartyId) as puid").distinct()

    fpids_2 = link_event_base.where("accounttype = 3 AND targetaccounttype = 2") \
        .selectExpr("targetaccountId as platform_id", f"dataanalytics{environment}.cdp_ng.fpid_decryption(firstpartyId) as puid").distinct()

    fpids_3 = dataframes["player_activity"].alias("pa").join(dataframes["ctp_profile"].alias("cp"), expr("lower(pa.player_id) = lower(cp.public_id)")) \
        .where("cp.first_party_id IS NOT NULL").selectExpr("pa.player_id as platform_id", "cp.first_party_id as puid").distinct()

    fpids = fpids_1.union(fpids_2).union(fpids_3).distinct()

    final_df = all_events.alias("a").join(fpids.alias("ctp"), expr("a.player_id = ctp.platform_id")) \
        .join(dataframes["pga_summary"].alias("sum"), expr("ctp.puid = sum.puid")) \
        .selectExpr(
            "a.EVENT_DATE",
            f"concat(a.vendor, ':', dataanalytics{environment}.cdp_ng.SALTED_PUID(ctp.puid)) as BRAND_FIRSTPARTYID",
            f"dataanalytics{environment}.cdp_ng.SALTED_PUID(ctp.puid) as SALTED_PUID",
            "ctp.puid", "a.vendor", "a.GAME_TITLE", "a.SUB_GAME", "a.EVENT_NAME", "a.EVENT_ACTION",
            "CAST(CAST(a.EVENT_RESULT AS DECIMAL(20, 2)) AS STRING) as EVENT_RESULT",
            "a.event_description",
            f"dataanalytics{environment}.cdp_ng.MK_CDP_FIELD2(a.GAME_TITLE, a.SUB_GAME, a.EVENT_NAME, a.EVENT_ACTION) as CDP_FIELD_NAME"
        ).distinct()
        
    return final_df

# COMMAND ----------

def load(df, table_name):

    df_with_ts = df.withColumn("dw_update_ts", expr("current_timestamp()")).withColumn("dw_insert_ts", expr("current_timestamp()"))
    df_to_load = df_with_ts.withColumn("merge_key", expr("""sha2(concat_ws('|', 
            EVENT_DATE, 
            BRAND_FIRSTPARTYID, 
            SALTED_PUID, 
            puid, 
            vendor, 
            GAME_TITLE, 
            SUB_GAME, 
            EVENT_NAME, 
            coalesce(EVENT_ACTION, 'NA'), 
            coalesce(EVENT_RESULT, 'NA'), 
            coalesce(event_description, 'NA'), 
            CDP_FIELD_NAME
        ), 256)
        """))
    merge_condition = "target.merge_key = source.merge_key"

    delta_table = DeltaTable.forName(spark, table_name)
    delta_table.alias("target").merge(df_to_load.alias("source"), merge_condition) \
        .whenNotMatchedInsertAll().execute()

def run_batch(load_type, incremental_days, target_table):

    create_pga2k25_daily_events(environment, spark)

    prev_max_ts = date(1990, 1, 1)
    if load_type == 'incremental':
        prev_max_ts = max_timestamp(spark, target_table, "dw_update_ts")

    source_dataframes = extract(spark, load_type, incremental_days, prev_max_ts)
    final_df = transform(source_dataframes)
    load(final_df, target_table)

# COMMAND ----------

run_batch(load_type, incremental_days, target_table)

# COMMAND ----------