# Databricks notebook source
# MAGIC %run ../table_functions

# COMMAND ----------

def create_agg_game_status_daily(spark, database, view_mapping, properties={}):
    """
    Create the agg_game_status_daily table in the specified environment.

    This function constructs a SQL command to create the agg_game_status_daily 
    table with predefined columns and executes it using the create_table function.
    create_table and environment are initiated as part of the table_functions

    Parameters:
    Input
        spark (SparkSession): The Spark session for executing the SQL command.
        title (str): The base title for the table (e.g., the database name).
        properties (dict, optional): A dictionary of properties to add to the table.
    Output
        A checkpoint location should be used for your checkpoint location.

    Example:
    Running it in a dev workspace
    agg_game_status_daily(spark, 'wwe2k25', {'delta.enableIcebergCompatV2': 'true'})
    Output:  "dbfs:/tmp/wwe2k25/intermediate/streaming/run_dev/agg_game_status_daily"
    """
    
    sql = f"""
            CREATE TABLE IF NOT EXISTS {database}{environment}.managed.agg_game_status_daily (
                date DATE comment 'date value from fact_player_game_status_daily',
                platform STRING  comment 'platform value from fact_player_game_status_daily',
                service STRING  comment 'service value from fact_player_game_status_daily',
                game_mode STRING comment 'gamemode value from fact_player_game_status_daily',
                sub_mode STRING comment 'submode value from fact_player_game_status_daily',
                country_code STRING,
                extra_grain_1 STRING DEFAULT 'Unused',
                agg_game_status_1 BIGINT DEFAULT -1,
                agg_game_status_2 BIGINT DEFAULT -1,
                agg_game_status_3 BIGINT DEFAULT -1,
                agg_game_status_4 BIGINT DEFAULT -1,
                agg_game_status_5 BIGINT DEFAULT -1,
                player_count DECIMAL(38,0),
                agg_gp_1 BIGINT DEFAULT -1,
                agg_gp_2 BIGINT DEFAULT -1,
                agg_gp_3 BIGINT DEFAULT -1,
                agg_gp_4 BIGINT DEFAULT -1,
                agg_gp_5 BIGINT DEFAULT -1,
                agg_gp_6 BIGINT DEFAULT -1,
                agg_gp_7 BIGINT DEFAULT -1,
                agg_gp_8 BIGINT DEFAULT -1,
                agg_gp_9 BIGINT DEFAULT -1,
                agg_gp_10 BIGINT DEFAULT -1,
                agg_gp_11 BIGINT DEFAULT -1,
                agg_gp_12 BIGINT DEFAULT -1,
                agg_gp_13 BIGINT DEFAULT -1,
                agg_gp_14 BIGINT DEFAULT -1,
                agg_gp_15 BIGINT DEFAULT -1,
                dw_insert_ts timestamp comment 'data warehouse audit field for records inserted timestamp',
                dw_update_ts timestamp comment 'data warehouse audit field for records updated timestamp',
                merge_key string comment 'unique id generated by the hash of the grain of the table '
            )
            comment 'daily game status details'
            partitioned by (platform,date)
    """
    properties = {
        "delta.feature.allowColumnDefaults": "supported" # this won't work until iceberg v3 is released
    }

    create_table(spark, sql, properties)
    create_agg_game_status_daily_view(spark, database, view_mapping)
    return f"dbfs:/tmp/{database}/managed/batch/run{environment}/agg_game_status_daily"


# COMMAND ----------

def create_agg_game_status_daily_view(spark, database, mapping):
    """
    Create the agg_game_status_daily view in the specified environment.

    Parameters:
        spark (SparkSession): The Spark session for executing the SQL command.
        database (str): The database to create the view in.
        mapping (dict): A dictionary of column mappings.
    """
    sql = f"""
    CREATE VIEW IF NOT EXISTS {database}{environment}.managed_view.agg_game_status_daily AS (
        SELECT
            date,
            platform,
            service,
            game_mode,
            sub_mode,
            country_code,
            player_count,
            {','.join(str(mapping[key]) for key in mapping)}
        from {database}{environment}.managed.agg_game_status_daily
    )
    """
    spark.sql(sql)
