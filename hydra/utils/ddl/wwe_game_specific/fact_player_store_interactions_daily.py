# Databricks notebook source
# MAGIC %run ../table_functions

# COMMAND ----------

def create_fact_player_store_interactions_daily(spark, database, view_mapping, properties={}):
    """
    Create the fact_player_store_interactions_daily table in the specified environment.

    This function constructs a SQL command to create the fact_player_store_interactions_daily 
    table with predefined columns and executes it using the create_table function.
    create_table and environment are initiated as part of the table_functions

    Parameters:
    Input
        spark (SparkSession): The Spark session for executing the SQL command.
        title (str): The base title for the table (e.g., the database name).
        properties (dict, optional): A dictionary of properties to add to the table.
    Output
        A checkpoint location should be used for your checkpoint location.

    Example:
    Running it in a dev workspace
    create_fact_player_store_interactions_daily(spark, 'wwe2k25', {'delta.enableIcebergCompatV2': 'true'})
    Output:  "dbfs:/tmp/wwe2k25/intermediate/streaming/run_dev/fact_player_store_interactions_daily"
    """
    
    sql = f"""
            CREATE TABLE IF NOT EXISTS {database}{environment}.managed.fact_player_store_interactions_daily (
                player_id string  comment 'player identifier from the source',
                platform string  comment 'transformed platform value which can be used for reporting',
                service string  comment 'transformed service value which can be used for reporting',
                date date comment 'received date from the source',
                game_mode string comment 'gamemode from the source',
                sub_mode string comment 'submode from the source',
                store_action string comment 'storeaction from the source',
                item string comment 'itemid from the source',
                item_name string DEFAULT 'Default' comment 'Default value to populate',
                item_type string DEFAULT 'Default' comment 'Default value to populate',
                time_spent decimal(38,0) comment 'rounded sum of secondsfloat from the source',
                items_purchased decimal(38,0) comment 'sum of numitemspurchased from the source',
                items_viewed decimal(38,0) comment 'sum of numitemsviewed from the source',
                agg_si_1 decimal(38,0) comment 'dummy null field',
                agg_si_2 decimal(38,0) comment 'dummy null field',
                agg_si_3 decimal(38,0) comment 'dummy null field',
                agg_si_4 decimal(38,0) comment 'dummy null field',
                agg_si_5 decimal(38,0) comment 'dummy null field',
                dw_insert_ts timestamp comment 'data warehouse audit field for records inserted timestamp',
                dw_update_ts timestamp comment 'data warehouse audit field for records updated timestamp',
                merge_key string comment 'unique id generated by the hash of the grain of the table '
            )
            comment 'player store interaction daily table'
            partitioned by (platform,date)
    """
    properties = {
        "delta.feature.allowColumnDefaults": "supported" # this won't work until iceberg v3 is released
    }

    create_table(spark, sql, properties)
    create_fact_player_store_interactions_daily_view(spark, database, view_mapping)
    return f"dbfs:/tmp/{database}/managed/batch/run{environment}/fact_player_store_interactions_daily"


# COMMAND ----------

def create_fact_player_store_interactions_daily_view(spark, database, mapping):
    """
    Create the fact_player_store_interactions_daily view in the specified environment.

    Parameters:
        spark (SparkSession): The Spark session for executing the SQL command.
        database (str): The database to create the view in.
        mapping (dict): A dictionary of column mappings.
    """
    sql = f"""
    CREATE VIEW IF NOT EXISTS {database}{environment}.managed_view.fact_player_store_interactions_daily AS (
        SELECT
            player_id,
            platform,
            service,
            date,
            game_mode,
            sub_mode,
            store_action,
            item,
            item_name,
            item_type,
            time_spent,
            items_purchased,
            items_viewed
        from {database}{environment}.managed.fact_player_store_interactions_daily
    )
    """
    spark.sql(sql)
